{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorSlicer\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='5.1,3.5,1.4,0.2,Iris-setosa'),\n",
       " Row(value='4.9,3.0,1.4,0.2,Iris-setosa'),\n",
       " Row(value='4.7,3.2,1.3,0.2,Iris-setosa'),\n",
       " Row(value='4.6,3.1,1.5,0.2,Iris-setosa'),\n",
       " Row(value='5.0,3.6,1.4,0.2,Iris-setosa')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = spark.read.text('C:\\spark\\data\\my_data\\iris_data.txt')\n",
    "train_data.head(5)\n",
    "# type(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|5.1,3.5,1.4,0.2,I...|\n",
      "|4.9,3.0,1.4,0.2,I...|\n",
      "|4.7,3.2,1.3,0.2,I...|\n",
      "|4.6,3.1,1.5,0.2,I...|\n",
      "|5.0,3.6,1.4,0.2,I...|\n",
      "|5.4,3.9,1.7,0.4,I...|\n",
      "|4.6,3.4,1.4,0.3,I...|\n",
      "|5.0,3.4,1.5,0.2,I...|\n",
      "|4.4,2.9,1.4,0.2,I...|\n",
      "|4.9,3.1,1.5,0.1,I...|\n",
      "|5.4,3.7,1.5,0.2,I...|\n",
      "|4.8,3.4,1.6,0.2,I...|\n",
      "|4.8,3.0,1.4,0.1,I...|\n",
      "|4.3,3.0,1.1,0.1,I...|\n",
      "|5.8,4.0,1.2,0.2,I...|\n",
      "|5.7,4.4,1.5,0.4,I...|\n",
      "|5.4,3.9,1.3,0.4,I...|\n",
      "|5.1,3.5,1.4,0.3,I...|\n",
      "|5.7,3.8,1.7,0.3,I...|\n",
      "|5.1,3.8,1.5,0.3,I...|\n",
      "|5.4,3.4,1.7,0.2,I...|\n",
      "|5.1,3.7,1.5,0.4,I...|\n",
      "|4.6,3.6,1.0,0.2,I...|\n",
      "|5.1,3.3,1.7,0.5,I...|\n",
      "|4.8,3.4,1.9,0.2,I...|\n",
      "|5.0,3.0,1.6,0.2,I...|\n",
      "|5.0,3.4,1.6,0.4,I...|\n",
      "|5.2,3.5,1.5,0.2,I...|\n",
      "|5.2,3.4,1.4,0.2,I...|\n",
      "|4.7,3.2,1.6,0.2,I...|\n",
      "|4.8,3.1,1.6,0.2,I...|\n",
      "|5.4,3.4,1.5,0.4,I...|\n",
      "|5.2,4.1,1.5,0.1,I...|\n",
      "|5.5,4.2,1.4,0.2,I...|\n",
      "|4.9,3.1,1.5,0.1,I...|\n",
      "|5.0,3.2,1.2,0.2,I...|\n",
      "|5.5,3.5,1.3,0.2,I...|\n",
      "|4.9,3.1,1.5,0.1,I...|\n",
      "|4.4,3.0,1.3,0.2,I...|\n",
      "|5.1,3.4,1.5,0.2,I...|\n",
      "|5.0,3.5,1.3,0.3,I...|\n",
      "|4.5,2.3,1.3,0.3,I...|\n",
      "|4.4,3.2,1.3,0.2,I...|\n",
      "|5.0,3.5,1.6,0.6,I...|\n",
      "|5.1,3.8,1.9,0.4,I...|\n",
      "|4.8,3.0,1.4,0.3,I...|\n",
      "|5.1,3.8,1.6,0.2,I...|\n",
      "|4.6,3.2,1.4,0.2,I...|\n",
      "|5.3,3.7,1.5,0.2,I...|\n",
      "|5.0,3.3,1.4,0.2,I...|\n",
      "|7.0,3.2,4.7,1.4,I...|\n",
      "|6.4,3.2,4.5,1.5,I...|\n",
      "|6.9,3.1,4.9,1.5,I...|\n",
      "|5.5,2.3,4.0,1.3,I...|\n",
      "|6.5,2.8,4.6,1.5,I...|\n",
      "|5.7,2.8,4.5,1.3,I...|\n",
      "|6.3,3.3,4.7,1.6,I...|\n",
      "|4.9,2.4,3.3,1.0,I...|\n",
      "|6.6,2.9,4.6,1.3,I...|\n",
      "|5.2,2.7,3.9,1.4,I...|\n",
      "|5.0,2.0,3.5,1.0,I...|\n",
      "|5.9,3.0,4.2,1.5,I...|\n",
      "|6.0,2.2,4.0,1.0,I...|\n",
      "|6.1,2.9,4.7,1.4,I...|\n",
      "|5.6,2.9,3.6,1.3,I...|\n",
      "|6.7,3.1,4.4,1.4,I...|\n",
      "|5.6,3.0,4.5,1.5,I...|\n",
      "|5.8,2.7,4.1,1.0,I...|\n",
      "|6.2,2.2,4.5,1.5,I...|\n",
      "|5.6,2.5,3.9,1.1,I...|\n",
      "|5.9,3.2,4.8,1.8,I...|\n",
      "|6.1,2.8,4.0,1.3,I...|\n",
      "|6.3,2.5,4.9,1.5,I...|\n",
      "|6.1,2.8,4.7,1.2,I...|\n",
      "|6.4,2.9,4.3,1.3,I...|\n",
      "|6.6,3.0,4.4,1.4,I...|\n",
      "|6.8,2.8,4.8,1.4,I...|\n",
      "|6.7,3.0,5.0,1.7,I...|\n",
      "|6.0,2.9,4.5,1.5,I...|\n",
      "|5.7,2.6,3.5,1.0,I...|\n",
      "|5.5,2.4,3.8,1.1,I...|\n",
      "|5.5,2.4,3.7,1.0,I...|\n",
      "|5.8,2.7,3.9,1.2,I...|\n",
      "|6.0,2.7,5.1,1.6,I...|\n",
      "|5.4,3.0,4.5,1.5,I...|\n",
      "|6.0,3.4,4.5,1.6,I...|\n",
      "|6.7,3.1,4.7,1.5,I...|\n",
      "|6.3,2.3,4.4,1.3,I...|\n",
      "|5.6,3.0,4.1,1.3,I...|\n",
      "|5.5,2.5,4.0,1.3,I...|\n",
      "|5.5,2.6,4.4,1.2,I...|\n",
      "|6.1,3.0,4.6,1.4,I...|\n",
      "|5.8,2.6,4.0,1.2,I...|\n",
      "|5.0,2.3,3.3,1.0,I...|\n",
      "|5.6,2.7,4.2,1.3,I...|\n",
      "|5.7,3.0,4.2,1.2,I...|\n",
      "|5.7,2.9,4.2,1.3,I...|\n",
      "|6.2,2.9,4.3,1.3,I...|\n",
      "|5.1,2.5,3.0,1.1,I...|\n",
      "|5.7,2.8,4.1,1.3,I...|\n",
      "+--------------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.show(100)  # 自动编码？？66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`_c4`' given input columns: [value];;\\n'Aggregate ['_c4], ['_c4]\\n+- AnalysisBarrier\\n      +- Relation[value#0] text\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o40.avg.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`_c4`' given input columns: [value];;\n'Aggregate ['_c4], ['_c4]\n+- AnalysisBarrier\n      +- Relation[value#0] text\n\r\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:106)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:118)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:122)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:122)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:127)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:95)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:80)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:80)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:92)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\r\n\tat org.apache.spark.sql.RelationalGroupedDataset.toDF(RelationalGroupedDataset.scala:66)\r\n\tat org.apache.spark.sql.RelationalGroupedDataset.aggregateNumericColumns(RelationalGroupedDataset.scala:110)\r\n\tat org.apache.spark.sql.RelationalGroupedDataset.avg(RelationalGroupedDataset.scala:274)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-0c4196100592>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_c4'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\group.py\u001b[0m in \u001b[0;36m_api\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_api\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: \"cannot resolve '`_c4`' given input columns: [value];;\\n'Aggregate ['_c4], ['_c4]\\n+- AnalysisBarrier\\n      +- Relation[value#0] text\\n\""
     ]
    }
   ],
   "source": [
    "train_data.groupby('_c4').avg().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='5.1,3.5,1.4,0.2,Iris-setosa'),\n",
       " Row(value='4.9,3.0,1.4,0.2,Iris-setosa'),\n",
       " Row(value='4.7,3.2,1.3,0.2,Iris-setosa'),\n",
       " Row(value='4.6,3.1,1.5,0.2,Iris-setosa'),\n",
       " Row(value='5.0,3.6,1.4,0.2,Iris-setosa'),\n",
       " Row(value='5.4,3.9,1.7,0.4,Iris-setosa'),\n",
       " Row(value='4.6,3.4,1.4,0.3,Iris-setosa'),\n",
       " Row(value='5.0,3.4,1.5,0.2,Iris-setosa'),\n",
       " Row(value='4.4,2.9,1.4,0.2,Iris-setosa'),\n",
       " Row(value='4.9,3.1,1.5,0.1,Iris-setosa'),\n",
       " Row(value='5.4,3.7,1.5,0.2,Iris-setosa'),\n",
       " Row(value='4.8,3.4,1.6,0.2,Iris-setosa'),\n",
       " Row(value='4.8,3.0,1.4,0.1,Iris-setosa'),\n",
       " Row(value='4.3,3.0,1.1,0.1,Iris-setosa'),\n",
       " Row(value='5.8,4.0,1.2,0.2,Iris-setosa'),\n",
       " Row(value='5.7,4.4,1.5,0.4,Iris-setosa'),\n",
       " Row(value='5.4,3.9,1.3,0.4,Iris-setosa'),\n",
       " Row(value='5.1,3.5,1.4,0.3,Iris-setosa'),\n",
       " Row(value='5.7,3.8,1.7,0.3,Iris-setosa'),\n",
       " Row(value='5.1,3.8,1.5,0.3,Iris-setosa'),\n",
       " Row(value='5.4,3.4,1.7,0.2,Iris-setosa'),\n",
       " Row(value='5.1,3.7,1.5,0.4,Iris-setosa'),\n",
       " Row(value='4.6,3.6,1.0,0.2,Iris-setosa'),\n",
       " Row(value='5.1,3.3,1.7,0.5,Iris-setosa'),\n",
       " Row(value='4.8,3.4,1.9,0.2,Iris-setosa'),\n",
       " Row(value='5.0,3.0,1.6,0.2,Iris-setosa'),\n",
       " Row(value='5.0,3.4,1.6,0.4,Iris-setosa'),\n",
       " Row(value='5.2,3.5,1.5,0.2,Iris-setosa'),\n",
       " Row(value='5.2,3.4,1.4,0.2,Iris-setosa'),\n",
       " Row(value='4.7,3.2,1.6,0.2,Iris-setosa'),\n",
       " Row(value='4.8,3.1,1.6,0.2,Iris-setosa'),\n",
       " Row(value='5.4,3.4,1.5,0.4,Iris-setosa'),\n",
       " Row(value='5.2,4.1,1.5,0.1,Iris-setosa'),\n",
       " Row(value='5.5,4.2,1.4,0.2,Iris-setosa'),\n",
       " Row(value='4.9,3.1,1.5,0.1,Iris-setosa'),\n",
       " Row(value='5.0,3.2,1.2,0.2,Iris-setosa'),\n",
       " Row(value='5.5,3.5,1.3,0.2,Iris-setosa'),\n",
       " Row(value='4.9,3.1,1.5,0.1,Iris-setosa'),\n",
       " Row(value='4.4,3.0,1.3,0.2,Iris-setosa'),\n",
       " Row(value='5.1,3.4,1.5,0.2,Iris-setosa'),\n",
       " Row(value='5.0,3.5,1.3,0.3,Iris-setosa'),\n",
       " Row(value='4.5,2.3,1.3,0.3,Iris-setosa'),\n",
       " Row(value='4.4,3.2,1.3,0.2,Iris-setosa'),\n",
       " Row(value='5.0,3.5,1.6,0.6,Iris-setosa'),\n",
       " Row(value='5.1,3.8,1.9,0.4,Iris-setosa'),\n",
       " Row(value='4.8,3.0,1.4,0.3,Iris-setosa'),\n",
       " Row(value='5.1,3.8,1.6,0.2,Iris-setosa'),\n",
       " Row(value='4.6,3.2,1.4,0.2,Iris-setosa'),\n",
       " Row(value='5.3,3.7,1.5,0.2,Iris-setosa'),\n",
       " Row(value='5.0,3.3,1.4,0.2,Iris-setosa'),\n",
       " Row(value='7.0,3.2,4.7,1.4,Iris-versicolor'),\n",
       " Row(value='6.4,3.2,4.5,1.5,Iris-versicolor'),\n",
       " Row(value='6.9,3.1,4.9,1.5,Iris-versicolor'),\n",
       " Row(value='5.5,2.3,4.0,1.3,Iris-versicolor'),\n",
       " Row(value='6.5,2.8,4.6,1.5,Iris-versicolor'),\n",
       " Row(value='5.7,2.8,4.5,1.3,Iris-versicolor'),\n",
       " Row(value='6.3,3.3,4.7,1.6,Iris-versicolor'),\n",
       " Row(value='4.9,2.4,3.3,1.0,Iris-versicolor'),\n",
       " Row(value='6.6,2.9,4.6,1.3,Iris-versicolor'),\n",
       " Row(value='5.2,2.7,3.9,1.4,Iris-versicolor'),\n",
       " Row(value='5.0,2.0,3.5,1.0,Iris-versicolor'),\n",
       " Row(value='5.9,3.0,4.2,1.5,Iris-versicolor'),\n",
       " Row(value='6.0,2.2,4.0,1.0,Iris-versicolor'),\n",
       " Row(value='6.1,2.9,4.7,1.4,Iris-versicolor'),\n",
       " Row(value='5.6,2.9,3.6,1.3,Iris-versicolor'),\n",
       " Row(value='6.7,3.1,4.4,1.4,Iris-versicolor'),\n",
       " Row(value='5.6,3.0,4.5,1.5,Iris-versicolor'),\n",
       " Row(value='5.8,2.7,4.1,1.0,Iris-versicolor'),\n",
       " Row(value='6.2,2.2,4.5,1.5,Iris-versicolor'),\n",
       " Row(value='5.6,2.5,3.9,1.1,Iris-versicolor'),\n",
       " Row(value='5.9,3.2,4.8,1.8,Iris-versicolor'),\n",
       " Row(value='6.1,2.8,4.0,1.3,Iris-versicolor'),\n",
       " Row(value='6.3,2.5,4.9,1.5,Iris-versicolor'),\n",
       " Row(value='6.1,2.8,4.7,1.2,Iris-versicolor'),\n",
       " Row(value='6.4,2.9,4.3,1.3,Iris-versicolor'),\n",
       " Row(value='6.6,3.0,4.4,1.4,Iris-versicolor'),\n",
       " Row(value='6.8,2.8,4.8,1.4,Iris-versicolor'),\n",
       " Row(value='6.7,3.0,5.0,1.7,Iris-versicolor'),\n",
       " Row(value='6.0,2.9,4.5,1.5,Iris-versicolor'),\n",
       " Row(value='5.7,2.6,3.5,1.0,Iris-versicolor'),\n",
       " Row(value='5.5,2.4,3.8,1.1,Iris-versicolor'),\n",
       " Row(value='5.5,2.4,3.7,1.0,Iris-versicolor'),\n",
       " Row(value='5.8,2.7,3.9,1.2,Iris-versicolor'),\n",
       " Row(value='6.0,2.7,5.1,1.6,Iris-versicolor'),\n",
       " Row(value='5.4,3.0,4.5,1.5,Iris-versicolor'),\n",
       " Row(value='6.0,3.4,4.5,1.6,Iris-versicolor'),\n",
       " Row(value='6.7,3.1,4.7,1.5,Iris-versicolor'),\n",
       " Row(value='6.3,2.3,4.4,1.3,Iris-versicolor'),\n",
       " Row(value='5.6,3.0,4.1,1.3,Iris-versicolor'),\n",
       " Row(value='5.5,2.5,4.0,1.3,Iris-versicolor'),\n",
       " Row(value='5.5,2.6,4.4,1.2,Iris-versicolor'),\n",
       " Row(value='6.1,3.0,4.6,1.4,Iris-versicolor'),\n",
       " Row(value='5.8,2.6,4.0,1.2,Iris-versicolor'),\n",
       " Row(value='5.0,2.3,3.3,1.0,Iris-versicolor'),\n",
       " Row(value='5.6,2.7,4.2,1.3,Iris-versicolor'),\n",
       " Row(value='5.7,3.0,4.2,1.2,Iris-versicolor'),\n",
       " Row(value='5.7,2.9,4.2,1.3,Iris-versicolor'),\n",
       " Row(value='6.2,2.9,4.3,1.3,Iris-versicolor'),\n",
       " Row(value='5.1,2.5,3.0,1.1,Iris-versicolor'),\n",
       " Row(value='5.7,2.8,4.1,1.3,Iris-versicolor'),\n",
       " Row(value='6.3,3.3,6.0,2.5,Iris-virginica'),\n",
       " Row(value='5.8,2.7,5.1,1.9,Iris-virginica'),\n",
       " Row(value='7.1,3.0,5.9,2.1,Iris-virginica'),\n",
       " Row(value='6.3,2.9,5.6,1.8,Iris-virginica'),\n",
       " Row(value='6.5,3.0,5.8,2.2,Iris-virginica'),\n",
       " Row(value='7.6,3.0,6.6,2.1,Iris-virginica'),\n",
       " Row(value='4.9,2.5,4.5,1.7,Iris-virginica'),\n",
       " Row(value='7.3,2.9,6.3,1.8,Iris-virginica'),\n",
       " Row(value='6.7,2.5,5.8,1.8,Iris-virginica'),\n",
       " Row(value='7.2,3.6,6.1,2.5,Iris-virginica'),\n",
       " Row(value='6.5,3.2,5.1,2.0,Iris-virginica'),\n",
       " Row(value='6.4,2.7,5.3,1.9,Iris-virginica'),\n",
       " Row(value='6.8,3.0,5.5,2.1,Iris-virginica'),\n",
       " Row(value='5.7,2.5,5.0,2.0,Iris-virginica'),\n",
       " Row(value='5.8,2.8,5.1,2.4,Iris-virginica'),\n",
       " Row(value='6.4,3.2,5.3,2.3,Iris-virginica'),\n",
       " Row(value='6.5,3.0,5.5,1.8,Iris-virginica'),\n",
       " Row(value='7.7,3.8,6.7,2.2,Iris-virginica'),\n",
       " Row(value='7.7,2.6,6.9,2.3,Iris-virginica'),\n",
       " Row(value='6.0,2.2,5.0,1.5,Iris-virginica'),\n",
       " Row(value='6.9,3.2,5.7,2.3,Iris-virginica'),\n",
       " Row(value='5.6,2.8,4.9,2.0,Iris-virginica'),\n",
       " Row(value='7.7,2.8,6.7,2.0,Iris-virginica'),\n",
       " Row(value='6.3,2.7,4.9,1.8,Iris-virginica'),\n",
       " Row(value='6.7,3.3,5.7,2.1,Iris-virginica'),\n",
       " Row(value='7.2,3.2,6.0,1.8,Iris-virginica'),\n",
       " Row(value='6.2,2.8,4.8,1.8,Iris-virginica'),\n",
       " Row(value='6.1,3.0,4.9,1.8,Iris-virginica'),\n",
       " Row(value='6.4,2.8,5.6,2.1,Iris-virginica'),\n",
       " Row(value='7.2,3.0,5.8,1.6,Iris-virginica'),\n",
       " Row(value='7.4,2.8,6.1,1.9,Iris-virginica'),\n",
       " Row(value='7.9,3.8,6.4,2.0,Iris-virginica'),\n",
       " Row(value='6.4,2.8,5.6,2.2,Iris-virginica'),\n",
       " Row(value='6.3,2.8,5.1,1.5,Iris-virginica'),\n",
       " Row(value='6.1,2.6,5.6,1.4,Iris-virginica'),\n",
       " Row(value='7.7,3.0,6.1,2.3,Iris-virginica'),\n",
       " Row(value='6.3,3.4,5.6,2.4,Iris-virginica'),\n",
       " Row(value='6.4,3.1,5.5,1.8,Iris-virginica'),\n",
       " Row(value='6.0,3.0,4.8,1.8,Iris-virginica'),\n",
       " Row(value='6.9,3.1,5.4,2.1,Iris-virginica'),\n",
       " Row(value='6.7,3.1,5.6,2.4,Iris-virginica'),\n",
       " Row(value='6.9,3.1,5.1,2.3,Iris-virginica'),\n",
       " Row(value='5.8,2.7,5.1,1.9,Iris-virginica'),\n",
       " Row(value='6.8,3.2,5.9,2.3,Iris-virginica'),\n",
       " Row(value='6.7,3.3,5.7,2.5,Iris-virginica'),\n",
       " Row(value='6.7,3.0,5.2,2.3,Iris-virginica'),\n",
       " Row(value='6.3,2.5,5.0,1.9,Iris-virginica'),\n",
       " Row(value='6.5,3.0,5.2,2.0,Iris-virginica'),\n",
       " Row(value='6.2,3.4,5.4,2.3,Iris-virginica'),\n",
       " Row(value='5.9,3.0,5.1,1.8,Iris-virginica')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0='5.1', _c1='3.5', _c2='1.4', _c3='0.2', _c4='1.0'),\n",
       " Row(_c0='4.9', _c1='3.0', _c2='1.4', _c3='0.2', _c4='1.0'),\n",
       " Row(_c0='4.7', _c1='3.2', _c2='1.3', _c3='0.2', _c4='1.0'),\n",
       " Row(_c0='4.6', _c1='3.1', _c2='1.5', _c3='0.2', _c4='1.0'),\n",
       " Row(_c0='5.0', _c1='3.6', _c2='1.4', _c3='0.2', _c4='1.0')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = spark.read.csv('C:\\spark\\data\\my_data\\iris_data.csv', header=False)\n",
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+\n",
      "|_c0|_c1|_c2|_c3|_c4|\n",
      "+---+---+---+---+---+\n",
      "|5.1|3.5|1.4|0.2|1.0|\n",
      "|4.9|3.0|1.4|0.2|1.0|\n",
      "|4.7|3.2|1.3|0.2|1.0|\n",
      "|4.6|3.1|1.5|0.2|1.0|\n",
      "|5.0|3.6|1.4|0.2|1.0|\n",
      "+---+---+---+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col = train_data['_c0'] * train_data['_c1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_data =train_data.withColumn('new_col', new_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, new_col: double]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+------------------+\n",
      "|_c0|_c1|_c2|_c3|_c4|           new_col|\n",
      "+---+---+---+---+---+------------------+\n",
      "|5.1|3.5|1.4|0.2|1.0|17.849999999999998|\n",
      "|4.9|3.0|1.4|0.2|1.0|14.700000000000001|\n",
      "|4.7|3.2|1.3|0.2|1.0|15.040000000000001|\n",
      "|4.6|3.1|1.5|0.2|1.0|             14.26|\n",
      "|5.0|3.6|1.4|0.2|1.0|              18.0|\n",
      "|5.4|3.9|1.7|0.4|1.0|21.060000000000002|\n",
      "|4.6|3.4|1.4|0.3|1.0|15.639999999999999|\n",
      "|5.0|3.4|1.5|0.2|1.0|              17.0|\n",
      "|4.4|2.9|1.4|0.2|1.0|             12.76|\n",
      "|4.9|3.1|1.5|0.1|1.0|15.190000000000001|\n",
      "|5.4|3.7|1.5|0.2|1.0|19.980000000000004|\n",
      "|4.8|3.4|1.6|0.2|1.0|             16.32|\n",
      "|4.8|3.0|1.4|0.1|1.0|14.399999999999999|\n",
      "|4.3|3.0|1.1|0.1|1.0|12.899999999999999|\n",
      "|5.8|4.0|1.2|0.2|1.0|              23.2|\n",
      "|5.7|4.4|1.5|0.4|1.0|25.080000000000002|\n",
      "|5.4|3.9|1.3|0.4|1.0|21.060000000000002|\n",
      "|5.1|3.5|1.4|0.3|1.0|17.849999999999998|\n",
      "|5.7|3.8|1.7|0.3|1.0|             21.66|\n",
      "|5.1|3.8|1.5|0.3|1.0|             19.38|\n",
      "+---+---+---+---+---+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_train_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+------------------+\n",
      "|_c0|_c1|_c2|_c3|_c4|           new_col|\n",
      "+---+---+---+---+---+------------------+\n",
      "|5.1|3.5|1.4|0.2|1.0|17.849999999999998|\n",
      "|4.9|3.0|1.4|0.2|1.0|14.700000000000001|\n",
      "|4.7|3.2|1.3|0.2|1.0|15.040000000000001|\n",
      "|4.6|3.1|1.5|0.2|1.0|             14.26|\n",
      "|5.0|3.6|1.4|0.2|1.0|              18.0|\n",
      "+---+---+---+---+---+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_result = new_train_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(show_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0', '_c1', '_c2', '_c3', '_c4']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|_c0|\n",
      "+---+\n",
      "|5.1|\n",
      "|4.9|\n",
      "|4.7|\n",
      "|4.6|\n",
      "|5.0|\n",
      "|5.4|\n",
      "|4.6|\n",
      "|5.0|\n",
      "|4.4|\n",
      "|4.9|\n",
      "+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.select('_c0').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = train_data.withColumn('_c0', train_data['_c0'].cast('double'))\\\n",
    "       .withColumn('_c1', train_data['_c1'].cast('double'))\\\n",
    "       .withColumn('_c2', train_data['_c2'].cast('double'))\\\n",
    "       .withColumn('_c3', train_data['_c3'].cast('double'))\n",
    "input_col = ['_c0', '_c1', '_c2', '_c3']\n",
    "vecAssembler = VectorAssembler(inputCols=input_col, outputCol=\"features\")\n",
    "new_train_data = vecAssembler.transform(train_data)\n",
    "# stringIndexer = StringIndexer(inputCol=\"_c4\", outputCol=\"label\")\n",
    "# pipeline = Pipeline(stages=[vecAssembler, stringIndexer])\n",
    "# pipelineFit = pipeline.fit(train_data)\n",
    "# dataset = pipelineFit.transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: double, _c1: double, _c2: double, _c3: double, _c4: string, features: vector]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=5.1, _c1=3.5, _c2=1.4, _c3=0.2, _c4='1.0', features=DenseVector([5.1, 3.5, 1.4, 0.2])),\n",
       " Row(_c0=4.9, _c1=3.0, _c2=1.4, _c3=0.2, _c4='1.0', features=DenseVector([4.9, 3.0, 1.4, 0.2]))]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_new_train_data = new_train_data.withColumn('label', new_train_data['_c4'].cast('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: double, _c1: double, _c2: double, _c3: double, _c4: string, features: vector, label: double]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_new_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+-----------------+-----+\n",
      "|_c0|_c1|_c2|_c3|_c4|         features|label|\n",
      "+---+---+---+---+---+-----------------+-----+\n",
      "|5.1|3.5|1.4|0.2|1.0|[5.1,3.5,1.4,0.2]|  1.0|\n",
      "|4.9|3.0|1.4|0.2|1.0|[4.9,3.0,1.4,0.2]|  1.0|\n",
      "|4.7|3.2|1.3|0.2|1.0|[4.7,3.2,1.3,0.2]|  1.0|\n",
      "|4.6|3.1|1.5|0.2|1.0|[4.6,3.1,1.5,0.2]|  1.0|\n",
      "|5.0|3.6|1.4|0.2|1.0|[5.0,3.6,1.4,0.2]|  1.0|\n",
      "+---+---+---+---+---+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_new_train_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+\n",
      "|_c0|_c1|_c2|_c3|_c4|\n",
      "+---+---+---+---+---+\n",
      "|5.1|3.5|1.4|0.2|1.0|\n",
      "|4.9|3.0|1.4|0.2|1.0|\n",
      "|4.7|3.2|1.3|0.2|1.0|\n",
      "|4.6|3.1|1.5|0.2|1.0|\n",
      "|5.0|3.6|1.4|0.2|1.0|\n",
      "+---+---+---+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新_模型训练\n",
    "lr_3 = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0)\n",
    "lrModel_3 = lr_3.fit(new_new_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_3 = lrModel_3.transform(new_new_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+-----------------+-----+--------------------+--------------------+----------+\n",
      "|_c0|_c1|_c2|_c3|_c4|         features|label|       rawPrediction|         probability|prediction|\n",
      "+---+---+---+---+---+-----------------+-----+--------------------+--------------------+----------+\n",
      "|5.1|3.5|1.4|0.2|1.0|[5.1,3.5,1.4,0.2]|  1.0|[-3.5921473185105...|[0.00157155276833...|       1.0|\n",
      "|4.9|3.0|1.4|0.2|1.0|[4.9,3.0,1.4,0.2]|  1.0|[-3.5335755527481...|[0.00215601550463...|       1.0|\n",
      "|4.7|3.2|1.3|0.2|1.0|[4.7,3.2,1.3,0.2]|  1.0|[-3.5417261943304...|[0.00184854030564...|       1.0|\n",
      "|4.6|3.1|1.5|0.2|1.0|[4.6,3.1,1.5,0.2]|  1.0|[-3.5284115349670...|[0.00201701076712...|       1.0|\n",
      "|5.0|3.6|1.4|0.2|1.0|[5.0,3.6,1.4,0.2]|  1.0|[-3.5966005410309...|[0.00144326216378...|       1.0|\n",
      "|5.4|3.9|1.7|0.4|1.0|[5.4,3.9,1.7,0.4]|  1.0|[-3.6511035819565...|[0.00135870845931...|       1.0|\n",
      "|4.6|3.4|1.4|0.3|1.0|[4.6,3.4,1.4,0.3]|  1.0|[-3.5578601196240...|[0.00167224197126...|       1.0|\n",
      "|5.0|3.4|1.5|0.2|1.0|[5.0,3.4,1.5,0.2]|  1.0|[-3.5780768556886...|[0.00169904494297...|       1.0|\n",
      "|4.4|2.9|1.4|0.2|1.0|[4.4,2.9,1.4,0.2]|  1.0|[-3.4980031989474...|[0.00222866869646...|       1.0|\n",
      "|4.9|3.1|1.5|0.1|1.0|[4.9,3.1,1.5,0.1]|  1.0|[-3.5426859456929...|[0.00199136561308...|       1.0|\n",
      "|5.4|3.7|1.5|0.2|1.0|[5.4,3.7,1.5,0.2]|  1.0|[-3.6277421764101...|[0.00141772976228...|       1.0|\n",
      "|4.8|3.4|1.6|0.2|1.0|[4.8,3.4,1.6,0.2]|  1.0|[-3.5684596153870...|[0.00169445886230...|       1.0|\n",
      "|4.8|3.0|1.4|0.1|1.0|[4.8,3.0,1.4,0.1]|  1.0|[-3.5271038759539...|[0.00207929376476...|       1.0|\n",
      "|4.3|3.0|1.1|0.1|1.0|[4.3,3.0,1.1,0.1]|  1.0|[-3.4989038561780...|[0.00192532173913...|       1.0|\n",
      "|5.8|4.0|1.2|0.2|1.0|[5.8,4.0,1.2,0.2]|  1.0|[-3.6751400867561...|[0.00111332240277...|       1.0|\n",
      "|5.7|4.4|1.5|0.4|1.0|[5.7,4.4,1.5,0.4]|  1.0|[-3.7133502626819...|[9.03078681020208...|       1.0|\n",
      "|5.4|3.9|1.3|0.4|1.0|[5.4,3.9,1.3,0.4]|  1.0|[-3.6480803681223...|[0.00127036007190...|       1.0|\n",
      "|5.1|3.5|1.4|0.3|1.0|[5.1,3.5,1.4,0.3]|  1.0|[-3.5934324734247...|[0.00163043654187...|       1.0|\n",
      "|5.7|3.8|1.7|0.3|1.0|[5.7,3.8,1.7,0.3]|  1.0|[-3.6557382482820...|[0.00145343844284...|       1.0|\n",
      "|5.1|3.8|1.5|0.3|1.0|[5.1,3.8,1.5,0.3]|  1.0|[-3.6231075100846...|[0.00132075328240...|       1.0|\n",
      "|5.4|3.4|1.7|0.2|1.0|[5.4,3.4,1.7,0.2]|  1.0|[-3.6003345501258...|[0.00180330959062...|       1.0|\n",
      "|5.1|3.7|1.5|0.4|1.0|[5.1,3.7,1.5,0.4]|  1.0|[-3.6147529205984...|[0.00148356199858...|       1.0|\n",
      "|4.6|3.6|1.0|0.2|1.0|[4.6,3.6,1.0,0.2]|  1.0|[-3.5728312396766...|[0.00128629648891...|       1.0|\n",
      "|5.1|3.3|1.7|0.5|1.0|[5.1,3.3,1.7,0.5]|  1.0|[-3.5789907048278...|[0.00207015643242...|       1.0|\n",
      "|4.8|3.4|1.9|0.2|1.0|[4.8,3.4,1.9,0.2]|  1.0|[-3.5707270257626...|[0.00177212942169...|       1.0|\n",
      "|5.0|3.0|1.6|0.2|1.0|[5.0,3.0,1.6,0.2]|  1.0|[-3.5402736815452...|[0.00221668900161...|       1.0|\n",
      "|5.0|3.4|1.6|0.4|1.0|[5.0,3.4,1.6,0.4]|  1.0|[-3.5814029689755...|[0.00184738550858...|       1.0|\n",
      "|5.2|3.5|1.5|0.2|1.0|[5.2,3.5,1.5,0.2]|  1.0|[-3.5980896438491...|[0.00161089801005...|       1.0|\n",
      "|5.2|3.4|1.4|0.2|1.0|[5.2,3.4,1.4,0.2]|  1.0|[-3.5876940959901...|[0.00170260306342...|       1.0|\n",
      "|4.7|3.2|1.6|0.2|1.0|[4.7,3.2,1.6,0.2]|  1.0|[-3.5439936047060...|[0.00192905899457...|       1.0|\n",
      "|4.8|3.1|1.6|0.2|1.0|[4.8,3.1,1.6,0.2]|  1.0|[-3.5395403821856...|[0.00207023164171...|       1.0|\n",
      "|5.4|3.4|1.5|0.4|1.0|[5.4,3.4,1.5,0.4]|  1.0|[-3.6013932530371...|[0.00187416399069...|       1.0|\n",
      "|5.2|4.1|1.5|0.1|1.0|[5.2,4.1,1.5,0.1]|  1.0|[-3.6546429553377...|[9.58707285920840...|       1.0|\n",
      "|5.5|4.2|1.4|0.2|1.0|[5.5,4.2,1.4,0.2]|  1.0|[-3.6803716168340...|[9.39308854541461...|       1.0|\n",
      "|4.9|3.1|1.5|0.1|1.0|[4.9,3.1,1.5,0.1]|  1.0|[-3.5426859456929...|[0.00199136561308...|       1.0|\n",
      "|5.0|3.2|1.2|0.2|1.0|[5.0,3.2,1.2,0.2]|  1.0|[-3.5565299565120...|[0.00186519461854...|       1.0|\n",
      "|5.5|3.5|1.3|0.2|1.0|[5.5,3.5,1.3,0.2]|  1.0|[-3.6121376025722...|[0.00160445078635...|       1.0|\n",
      "|4.9|3.1|1.5|0.1|1.0|[4.9,3.1,1.5,0.1]|  1.0|[-3.5426859456929...|[0.00199136561308...|       1.0|\n",
      "|4.4|3.0|1.3|0.2|1.0|[4.4,3.0,1.3,0.2]|  1.0|[-3.5068871398893...|[0.00206546866807...|       1.0|\n",
      "|5.1|3.4|1.5|0.2|1.0|[5.1,3.4,1.5,0.2]|  1.0|[-3.5832633775686...|[0.00171357274807...|       1.0|\n",
      "|5.0|3.5|1.3|0.3|1.0|[5.0,3.5,1.3,0.3]|  1.0|[-3.5874901480861...|[0.00159036077402...|       1.0|\n",
      "|4.5|2.3|1.3|0.3|1.0|[4.5,2.3,1.3,0.3]|  1.0|[-3.4458806058803...|[0.00301239041046...|       1.0|\n",
      "|4.4|3.2|1.3|0.2|1.0|[4.4,3.2,1.3,0.2]|  1.0|[-3.5261666286902...|[0.00180115378478...|       1.0|\n",
      "|5.0|3.5|1.6|0.6|1.0|[5.0,3.5,1.6,0.6]|  1.0|[-3.5936130232043...|[0.00184702456518...|       1.0|\n",
      "|5.1|3.8|1.9|0.4|1.0|[5.1,3.8,1.9,0.4]|  1.0|[-3.6274158788330...|[0.00146733804919...|       1.0|\n",
      "|4.8|3.0|1.4|0.3|1.0|[4.8,3.0,1.4,0.3]|  1.0|[-3.5296741857822...|[0.00221043738354...|       1.0|\n",
      "|5.1|3.8|1.6|0.2|1.0|[5.1,3.8,1.6,0.2]|  1.0|[-3.6225781586290...|[0.00129081209097...|       1.0|\n",
      "|4.6|3.2|1.4|0.2|1.0|[4.6,3.2,1.4,0.2]|  1.0|[-3.5372954759089...|[0.00186028148286...|       1.0|\n",
      "|5.3|3.7|1.5|0.2|1.0|[5.3,3.7,1.5,0.2]|  1.0|[-3.6225556545301...|[0.00140310479148...|       1.0|\n",
      "|5.0|3.3|1.4|0.2|1.0|[5.0,3.3,1.4,0.2]|  1.0|[-3.5676813078295...|[0.00179427413259...|       1.0|\n",
      "|7.0|3.2|4.7|1.4|2.0|[7.0,3.2,4.7,1.4]|  2.0|[-3.7021353741322...|[0.00213806025593...|       2.0|\n",
      "|6.4|3.2|4.5|1.5|2.0|[6.4,3.2,4.5,1.5]|  2.0|[-3.6707897908490...|[0.00232093029625...|       2.0|\n",
      "|6.9|3.1|4.9|1.5|2.0|[6.9,3.1,4.9,1.5]|  2.0|[-3.6901058696830...|[0.00207498636907...|       2.0|\n",
      "|5.5|2.3|4.0|1.3|2.0|[5.5,2.3,4.0,1.3]|  2.0|[-3.5310040672033...|[0.00253540053381...|       2.0|\n",
      "|6.5|2.8|4.6|1.5|2.0|[6.5,2.8,4.6,1.5]|  2.0|[-3.6381731385857...|[0.00215696871386...|       2.0|\n",
      "|5.7|2.8|4.5|1.3|2.0|[5.7,2.8,4.5,1.3]|  2.0|[-3.5933548502585...|[0.00252859177883...|       2.0|\n",
      "|6.3|3.3|4.7|1.6|2.0|[6.3,3.3,4.7,1.6]|  2.0|[-3.6780397752007...|[0.00230333369515...|       3.0|\n",
      "|4.9|2.4|3.3|1.0|2.0|[4.9,2.4,3.3,1.0]|  2.0|[-3.5003785913711...|[0.00302523581820...|       2.0|\n",
      "|6.6|2.9|4.6|1.3|2.0|[6.6,2.9,4.6,1.3]|  2.0|[-3.6504290950379...|[0.00223234803772...|       2.0|\n",
      "|5.2|2.7|3.9|1.4|2.0|[5.2,2.7,3.9,1.4]|  2.0|[-3.5545328306206...|[0.00274134492275...|       3.0|\n",
      "|5.0|2.0|3.5|1.0|2.0|[5.0,2.0,3.5,1.0]|  2.0|[-3.4685177425664...|[0.00286096023846...|       2.0|\n",
      "|5.9|3.0|4.2|1.5|2.0|[5.9,3.0,4.2,1.5]|  2.0|[-3.6233102822722...|[0.00248323497448...|       3.0|\n",
      "|6.0|2.2|4.0|1.0|2.0|[6.0,2.2,4.0,1.0]|  2.0|[-3.5434414674605...|[0.00241442833624...|       2.0|\n",
      "|6.1|2.9|4.7|1.4|2.0|[6.1,2.9,4.7,1.4]|  2.0|[-3.6265374440104...|[0.00234665527164...|       3.0|\n",
      "|5.6|2.9|3.6|1.3|2.0|[5.6,2.9,3.6,1.3]|  2.0|[-3.5910058416519...|[0.00270482198873...|       2.0|\n",
      "|6.7|3.1|4.4|1.4|2.0|[6.7,3.1,4.4,1.4]|  2.0|[-3.6746686537160...|[0.00225413549169...|       2.0|\n",
      "|5.6|3.0|4.5|1.5|2.0|[5.6,3.0,4.5,1.5]|  2.0|[-3.6100181270077...|[0.00252555782290...|       3.0|\n",
      "|5.8|2.7|4.1|1.0|2.0|[5.8,2.7,4.1,1.0]|  2.0|[-3.5820229491613...|[0.00263029403381...|       2.0|\n",
      "|6.2|2.2|4.5|1.5|2.0|[6.2,2.2,4.5,1.5]|  2.0|[-3.5640193030842...|[0.00202361497532...|       3.0|\n",
      "|5.6|2.5|3.9|1.1|2.0|[5.6,2.5,3.9,1.1]|  2.0|[-3.5521439645974...|[0.00266382475047...|       2.0|\n",
      "|5.9|3.2|4.8|1.8|2.0|[5.9,3.2,4.8,1.8]|  2.0|[-3.6509800565669...|[0.00229710568466...|       3.0|\n",
      "|6.1|2.8|4.0|1.3|2.0|[6.1,2.8,4.0,1.3]|  2.0|[-3.6103219204859...|[0.00247640141023...|       2.0|\n",
      "|6.3|2.5|4.9|1.5|2.0|[6.3,2.5,4.9,1.5]|  2.0|[-3.6011482719999...|[0.00204121095709...|       3.0|\n",
      "|6.1|2.8|4.7|1.2|2.0|[6.1,2.8,4.7,1.2]|  2.0|[-3.6143273897816...|[0.00239218251807...|       2.0|\n",
      "|6.4|2.9|4.3|1.3|2.0|[6.4,2.9,4.3,1.3]|  2.0|[-3.6377886409022...|[0.00235021408686...|       2.0|\n",
      "|6.6|3.0|4.4|1.4|2.0|[6.6,3.0,4.4,1.4]|  2.0|[-3.6598423874354...|[0.00226140139863...|       2.0|\n",
      "|6.8|2.8|4.8|1.4|2.0|[6.8,2.8,4.8,1.4]|  2.0|[-3.6539591562288...|[0.00205756938237...|       2.0|\n",
      "|6.7|3.0|5.0|1.7|2.0|[6.7,3.0,5.0,1.7]|  2.0|[-3.6734191948093...|[0.00200341596097...|       3.0|\n",
      "|6.0|2.9|4.5|1.5|2.0|[6.0,2.9,4.5,1.5]|  2.0|[-3.6211244701274...|[0.00237588178102...|       3.0|\n",
      "|5.7|2.6|3.5|1.0|2.0|[5.7,2.6,3.5,1.0]|  2.0|[-3.5626618621295...|[0.00273153367134...|       2.0|\n",
      "|5.5|2.4|3.8|1.1|2.0|[5.5,2.4,3.8,1.1]|  2.0|[-3.5365618948583...|[0.00269232450127...|       2.0|\n",
      "|5.5|2.4|3.7|1.0|2.0|[5.5,2.4,3.7,1.0]|  2.0|[-3.5345209364856...|[0.00274178646388...|       2.0|\n",
      "|5.8|2.7|3.9|1.2|2.0|[5.8,2.7,3.9,1.2]|  2.0|[-3.5830816520726...|[0.00260562788759...|       2.0|\n",
      "|6.0|2.7|5.1|1.6|2.0|[6.0,2.7,5.1,1.6]|  2.0|[-3.6076649569919...|[0.00213487154702...|       3.0|\n",
      "|5.4|3.0|4.5|1.5|2.0|[5.4,3.0,4.5,1.5]|  2.0|[-3.5996450832476...|[0.00258528810106...|       3.0|\n",
      "|6.0|3.4|4.5|1.6|2.0|[6.0,3.4,4.5,1.6]|  2.0|[-3.6706083470439...|[0.00241991621154...|       3.0|\n",
      "|6.7|3.1|4.7|1.5|2.0|[6.7,3.1,4.7,1.5]|  2.0|[-3.6782212190058...|[0.00217258783791...|       2.0|\n",
      "|6.3|2.3|4.4|1.3|2.0|[6.3,2.3,4.4,1.3]|  2.0|[-3.5755194560779...|[0.00214410092373...|       2.0|\n",
      "|5.6|3.0|4.1|1.3|2.0|[5.6,3.0,4.1,1.3]|  2.0|[-3.6044246033451...|[0.00264779095470...|       2.0|\n",
      "|5.5|2.5|4.0|1.3|2.0|[5.5,2.5,4.0,1.3]|  2.0|[-3.5502835560042...|[0.00260839839128...|       2.0|\n",
      "|5.5|2.6|4.4|1.2|2.0|[5.5,2.6,4.4,1.2]|  2.0|[-3.5616613593247...|[0.00260236954313...|       2.0|\n",
      "|6.1|3.0|4.6|1.4|2.0|[6.1,3.0,4.6,1.4]|  2.0|[-3.6354213849523...|[0.00239056964039...|       2.0|\n",
      "|5.8|2.6|4.0|1.2|2.0|[5.8,2.6,4.0,1.2]|  2.0|[-3.5741977111307...|[0.00256524183630...|       2.0|\n",
      "|5.0|2.3|3.3|1.0|2.0|[5.0,2.3,3.3,1.0]|  2.0|[-3.4959253688507...|[0.00297937439576...|       2.0|\n",
      "|5.6|2.7|4.2|1.3|2.0|[5.6,2.7,4.2,1.3]|  2.0|[-3.5762611736023...|[0.00259147276558...|       2.0|\n",
      "|5.7|3.0|4.2|1.2|2.0|[5.7,3.0,4.2,1.2]|  2.0|[-3.6090817737696...|[0.00263041361302...|       2.0|\n",
      "|5.7|2.9|4.2|1.3|2.0|[5.7,2.9,4.2,1.3]|  2.0|[-3.6007271842833...|[0.00259648884620...|       2.0|\n",
      "|6.2|2.9|4.3|1.3|2.0|[6.2,2.9,4.3,1.3]|  2.0|[-3.6274155971421...|[0.00241761555323...|       2.0|\n",
      "|5.1|2.5|3.0|1.1|2.0|[5.1,2.5,3.0,1.1]|  2.0|[-3.5194091240702...|[0.00297249146022...|       2.0|\n",
      "|5.7|2.8|4.1|1.3|2.0|[5.7,2.8,4.1,1.3]|  2.0|[-3.5903316364243...|[0.00259590683535...|       2.0|\n",
      "|6.3|3.3|6.0|2.5|3.0|[6.3,3.3,6.0,2.5]|  3.0|[-3.6994316143894...|[0.00159655466774...|       3.0|\n",
      "|5.8|2.7|5.1|1.9|3.0|[5.8,2.7,5.1,1.9]|  3.0|[-3.6011473779744...|[0.00203346640375...|       3.0|\n",
      "|7.1|3.0|5.9|2.1|3.0|[7.1,3.0,5.9,2.1]|  3.0|[-3.7061081331131...|[0.00153080039650...|       3.0|\n",
      "|6.3|2.9|5.6|1.8|3.0|[6.3,2.9,5.6,1.8]|  3.0|[-3.6488533385541...|[0.00191377971767...|       3.0|\n",
      "|6.5|3.0|5.8|2.2|3.0|[6.5,3.0,5.8,2.2]|  3.0|[-3.6755183532885...|[0.00164592798798...|       3.0|\n",
      "|7.6|3.0|6.6|2.1|3.0|[7.6,3.0,6.6,2.1]|  3.0|[-3.7373313667232...|[0.00129279018938...|       3.0|\n",
      "|4.9|2.5|4.5|1.7|3.0|[4.9,2.5,4.5,1.7]|  3.0|[-3.5280840616734...|[0.00250114153298...|       3.0|\n",
      "|7.3|2.9|6.3|1.8|3.0|[7.3,2.9,6.3,1.8]|  3.0|[-3.7060091815644...|[0.00150347994067...|       3.0|\n",
      "|6.7|2.5|5.8|1.8|3.0|[6.7,2.5,5.8,1.8]|  3.0|[-3.6325520553895...|[0.00159376591913...|       3.0|\n",
      "|7.2|3.6|6.1|2.5|3.0|[7.2,3.6,6.1,2.5]|  3.0|[-3.7757853479698...|[0.00149099247991...|       3.0|\n",
      "|6.5|3.2|5.1|2.0|3.0|[6.5,3.2,5.1,2.0]|  3.0|[-3.6869369080512...|[0.00197142988678...|       3.0|\n",
      "|6.4|2.7|5.3|1.9|3.0|[6.4,2.7,5.3,1.9]|  3.0|[-3.6337781161718...|[0.00181385813305...|       3.0|\n",
      "|6.8|3.0|5.5|2.1|3.0|[6.8,3.0,5.5,2.1]|  3.0|[-3.6875253536388...|[0.00168377463776...|       3.0|\n",
      "|5.7|2.5|5.0|2.0|3.0|[5.7,2.5,5.0,2.0]|  3.0|[-3.5772107187490...|[0.00193963727926...|       3.0|\n",
      "|5.8|2.8|5.1|2.4|3.0|[5.8,2.8,5.1,2.4]|  3.0|[-3.6172128969457...|[0.00177433225943...|       3.0|\n",
      "|6.4|3.2|5.3|2.3|3.0|[6.4,3.2,5.3,2.3]|  3.0|[-3.6871174578308...|[0.00179679376196...|       3.0|\n",
      "|6.5|3.0|5.5|1.8|3.0|[6.5,3.0,5.5,1.8]|  3.0|[-3.6681103232561...|[0.00191625429889...|       3.0|\n",
      "|7.7|3.8|6.7|2.2|3.0|[7.7,3.8,6.7,2.2]|  3.0|[-3.8216768021797...|[0.00148291268558...|       3.0|\n",
      "|7.7|2.6|6.9|2.3|3.0|[7.7,2.6,6.9,2.3]|  3.0|[-3.7087966312054...|[0.00102081651498...|       3.0|\n",
      "|6.0|2.2|5.0|1.5|3.0|[6.0,2.2,5.0,1.5]|  3.0|[-3.5574252766169...|[0.00198316035683...|       3.0|\n",
      "|6.9|3.2|5.7|2.3|3.0|[6.9,3.2,5.7,2.3]|  3.0|[-3.7160732810652...|[0.00159413623650...|       3.0|\n",
      "|5.6|2.8|4.9|2.0|3.0|[5.6,2.8,4.9,2.0]|  3.0|[-3.6001876266118...|[0.00212128374383...|       3.0|\n",
      "|7.7|2.8|6.7|2.0|3.0|[7.7,2.8,6.7,2.0]|  3.0|[-3.7227090483467...|[0.00122640045861...|       3.0|\n",
      "|6.3|2.7|4.9|1.8|3.0|[6.3,2.7,4.9,1.8]|  3.0|[-3.6242832255433...|[0.00198061624269...|       3.0|\n",
      "|6.7|3.3|5.7|2.1|3.0|[6.7,3.3,5.7,2.1]|  3.0|[-3.7127696718772...|[0.00178220158664...|       3.0|\n",
      "|7.2|3.2|6.0|1.8|3.0|[7.2,3.2,6.0,1.8]|  3.0|[-3.7274744825101...|[0.00169884613726...|       3.0|\n",
      "|6.2|2.8|4.8|1.8|3.0|[6.2,2.8,4.8,1.8]|  3.0|[-3.6279806446052...|[0.00207420421913...|       3.0|\n",
      "|6.1|3.0|4.9|1.8|3.0|[6.1,3.0,4.9,1.8]|  3.0|[-3.6428294149846...|[0.00215888320831...|       3.0|\n",
      "|6.4|2.8|5.6|2.1|3.0|[6.4,2.8,5.6,2.1]|  3.0|[-3.6482555807762...|[0.00168646913410...|       3.0|\n",
      "|7.2|3.0|5.8|1.6|3.0|[7.2,3.0,5.8,1.6]|  3.0|[-3.7041130769637...|[0.00174453830564...|       3.0|\n",
      "|7.4|2.8|6.1|1.9|3.0|[7.4,2.8,6.1,1.9]|  3.0|[-3.7013295070411...|[0.00143409969457...|       3.0|\n",
      "|7.9|3.8|6.4|2.0|3.0|[7.9,3.8,6.4,2.0]|  3.0|[-3.8272121257358...|[0.00157235477411...|       3.0|\n",
      "|6.4|2.8|5.6|2.2|3.0|[6.4,2.8,5.6,2.2]|  3.0|[-3.6495407356904...|[0.00163267877383...|       3.0|\n",
      "|6.3|2.8|5.1|1.5|3.0|[6.3,2.8,5.1,1.5]|  3.0|[-3.6315791121184...|[0.00212492537530...|       3.0|\n",
      "|6.1|2.6|5.6|1.4|3.0|[6.1,2.6,5.6,1.4]|  3.0|[-3.6044204419359...|[0.00205141106538...|       3.0|\n",
      "|7.7|3.0|6.1|2.3|3.0|[7.7,3.0,6.1,2.3]|  3.0|[-3.7413091811389...|[0.00127214280669...|       3.0|\n",
      "|6.3|3.4|5.6|2.4|3.0|[6.3,3.4,5.6,2.4]|  3.0|[-3.7047629900415...|[0.00177301921231...|       3.0|\n",
      "|6.4|3.1|5.5|1.8|3.0|[6.4,3.1,5.5,1.8]|  3.0|[-3.6725635457765...|[0.00198290265033...|       3.0|\n",
      "|6.0|3.0|4.8|1.8|3.0|[6.0,3.0,4.8,1.8]|  3.0|[-3.6368870896461...|[0.00220969453991...|       3.0|\n",
      "|6.9|3.1|5.4|2.1|3.0|[6.9,3.1,5.4,2.1]|  3.0|[-3.7015958164608...|[0.00171633397256...|       3.0|\n",
      "|6.7|3.1|5.6|2.4|3.0|[6.7,3.1,5.6,2.4]|  3.0|[-3.6965898443603...|[0.00157108091762...|       3.0|\n",
      "|6.9|3.1|5.1|2.3|3.0|[6.9,3.1,5.1,2.3]|  3.0|[-3.7018987159135...|[0.00167580567196...|       3.0|\n",
      "|5.8|2.7|5.1|1.9|3.0|[5.8,2.7,5.1,1.9]|  3.0|[-3.6011473779744...|[0.00203346640375...|       3.0|\n",
      "|6.8|3.2|5.9|2.3|3.0|[6.8,3.2,5.9,2.3]|  3.0|[-3.7123983661023...|[0.00157793845336...|       3.0|\n",
      "|6.7|3.3|5.7|2.5|3.0|[6.7,3.3,5.7,2.5]|  3.0|[-3.7179102915339...|[0.00157109560049...|       3.0|\n",
      "|6.7|3.0|5.2|2.3|3.0|[6.7,3.0,5.2,2.3]|  3.0|[-3.6826417312115...|[0.00166697960789...|       3.0|\n",
      "|6.3|2.5|5.0|1.9|3.0|[6.3,2.5,5.0,1.9]|  3.0|[-3.6070446951151...|[0.00182071826045...|       3.0|\n",
      "|6.5|3.0|5.2|2.0|3.0|[6.5,3.0,5.2,2.0]|  3.0|[-3.6684132227088...|[0.00187693270167...|       3.0|\n",
      "|6.2|3.4|5.4|2.3|3.0|[6.2,3.4,5.4,2.3]|  3.0|[-3.6967797063302...|[0.00189510211760...|       3.0|\n",
      "|5.9|3.0|5.1|1.8|3.0|[5.9,3.0,5.1,1.8]|  3.0|[-3.6339679781416...|[0.00217684800432...|       3.0|\n",
      "+---+---+---+---+---+-----------------+-----+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pre_3.show(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_3_test = pre_3.select(['_c0','probability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "|_c0|         probability|\n",
      "+---+--------------------+\n",
      "|5.1|[0.00157155276833...|\n",
      "|4.9|[0.00215601550463...|\n",
      "|4.7|[0.00184854030564...|\n",
      "|4.6|[0.00201701076712...|\n",
      "|5.0|[0.00144326216378...|\n",
      "|5.4|[0.00135870845931...|\n",
      "|4.6|[0.00167224197126...|\n",
      "|5.0|[0.00169904494297...|\n",
      "|4.4|[0.00222866869646...|\n",
      "|4.9|[0.00199136561308...|\n",
      "|5.4|[0.00141772976228...|\n",
      "|4.8|[0.00169445886230...|\n",
      "|4.8|[0.00207929376476...|\n",
      "|4.3|[0.00192532173913...|\n",
      "|5.8|[0.00111332240277...|\n",
      "|5.7|[9.03078681020208...|\n",
      "|5.4|[0.00127036007190...|\n",
      "|5.1|[0.00163043654187...|\n",
      "|5.7|[0.00145343844284...|\n",
      "|5.1|[0.00132075328240...|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pre_3_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0015715527683337984"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_3_test.select('probability').collect()[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(df):\n",
    "    df.withColumn('probb', df.select('probability').collect()[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(df):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 32.0 failed 1 times, most recent failure: Lost task 0.0 in stage 32.0 (TID 32, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\", line 1556, in __getattr__\n    idx = self.__fields__.index(item)\nValueError: 'withColumn' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 230, in main\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 225, in process\n  File \"C:\\spark\\python\\pyspark\\rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\spark\\python\\pyspark\\rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\spark\\python\\pyspark\\rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\spark\\python\\pyspark\\rdd.py\", line 370, in func\n    return f(iterator)\n  File \"C:\\spark\\python\\pyspark\\rdd.py\", line 805, in processPartition\n    f(x)\n  File \"C:\\spark\\python\\pyspark\\util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-44-7f1e5ecdbacf>\", line 2, in f\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\", line 1561, in __getattr__\n    raise AttributeError(item)\nAttributeError: withColumn\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:162)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\", line 1556, in __getattr__\n    idx = self.__fields__.index(item)\nValueError: 'withColumn' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 230, in main\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 225, in process\n  File \"C:\\spark\\python\\pyspark\\rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\spark\\python\\pyspark\\rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\spark\\python\\pyspark\\rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\spark\\python\\pyspark\\rdd.py\", line 370, in func\n    return f(iterator)\n  File \"C:\\spark\\python\\pyspark\\rdd.py\", line 805, in processPartition\n    f(x)\n  File \"C:\\spark\\python\\pyspark\\util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-44-7f1e5ecdbacf>\", line 2, in f\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\", line 1561, in __getattr__\n    raise AttributeError(item)\nAttributeError: withColumn\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-2282a2b8ebac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpre_3_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforeach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mforeach\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    514\u001b[0m         \u001b[1;33m>>\u001b[0m\u001b[1;33m>\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforeach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m         \"\"\"\n\u001b[1;32m--> 516\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforeach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mforeach\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    805\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessPartition\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Force evaluation\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforeachPartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcount\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1071\u001b[0m         \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1072\u001b[0m         \"\"\"\n\u001b[1;32m-> 1073\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1075\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1062\u001b[0m         \u001b[1;36m6.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1063\u001b[0m         \"\"\"\n\u001b[1;32m-> 1064\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1066\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mfold\u001b[1;34m(self, zeroValue, op)\u001b[0m\n\u001b[0;32m    933\u001b[0m         \u001b[1;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m         \u001b[1;31m# to the final reduce call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 935\u001b[1;33m         \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    936\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    937\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    832\u001b[0m         \"\"\"\n\u001b[0;32m    833\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 834\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    835\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    836\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 32.0 failed 1 times, most recent failure: Lost task 0.0 in stage 32.0 (TID 32, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\", line 1556, in __getattr__\n    idx = self.__fields__.index(item)\nValueError: 'withColumn' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 230, in main\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 225, in process\n  File \"C:\\spark\\python\\pyspark\\rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\spark\\python\\pyspark\\rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\spark\\python\\pyspark\\rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\spark\\python\\pyspark\\rdd.py\", line 370, in func\n    return f(iterator)\n  File \"C:\\spark\\python\\pyspark\\rdd.py\", line 805, in processPartition\n    f(x)\n  File \"C:\\spark\\python\\pyspark\\util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-44-7f1e5ecdbacf>\", line 2, in f\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\", line 1561, in __getattr__\n    raise AttributeError(item)\nAttributeError: withColumn\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:162)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\", line 1556, in __getattr__\n    idx = self.__fields__.index(item)\nValueError: 'withColumn' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 230, in main\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 225, in process\n  File \"C:\\spark\\python\\pyspark\\rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\spark\\python\\pyspark\\rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\spark\\python\\pyspark\\rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\spark\\python\\pyspark\\rdd.py\", line 370, in func\n    return f(iterator)\n  File \"C:\\spark\\python\\pyspark\\rdd.py\", line 805, in processPartition\n    f(x)\n  File \"C:\\spark\\python\\pyspark\\util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-44-7f1e5ecdbacf>\", line 2, in f\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\", line 1561, in __getattr__\n    raise AttributeError(item)\nAttributeError: withColumn\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "pre_3_test.foreach(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-8ee22912647e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpre_3_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpre_3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'probability'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoArray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1180\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1181\u001b[0m             raise AttributeError(\n\u001b[1;32m-> 1182\u001b[1;33m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[0;32m   1183\u001b[0m         \u001b[0mjc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1184\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "pre_3_new = pre_3.map(lambda x:x.select('probability').collect().toArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'probability'>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_3['probability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[probability: vector]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_3.select('probability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[probability: vector]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = pre_3.select('probability')\n",
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(probability=DenseVector([0.0016, 0.7914, 0.1504, 0.0567])),\n",
       " Row(probability=DenseVector([0.0022, 0.6804, 0.2289, 0.0885])),\n",
       " Row(probability=DenseVector([0.0018, 0.7571, 0.1722, 0.0689])),\n",
       " Row(probability=DenseVector([0.002, 0.7275, 0.1909, 0.0796])),\n",
       " Row(probability=DenseVector([0.0014, 0.8187, 0.1297, 0.0501])),\n",
       " Row(probability=DenseVector([0.0014, 0.8193, 0.1258, 0.0536])),\n",
       " Row(probability=DenseVector([0.0017, 0.7935, 0.1418, 0.063])),\n",
       " Row(probability=DenseVector([0.0017, 0.7704, 0.1641, 0.0638])),\n",
       " Row(probability=DenseVector([0.0022, 0.6971, 0.2109, 0.0899])),\n",
       " Row(probability=DenseVector([0.002, 0.7149, 0.207, 0.0761])),\n",
       " Row(probability=DenseVector([0.0014, 0.8083, 0.1398, 0.0506])),\n",
       " Row(probability=DenseVector([0.0017, 0.7795, 0.1552, 0.0636])),\n",
       " Row(probability=DenseVector([0.0021, 0.7029, 0.2154, 0.0796])),\n",
       " Row(probability=DenseVector([0.0019, 0.763, 0.1688, 0.0664])),\n",
       " Row(probability=DenseVector([0.0011, 0.8533, 0.1099, 0.0357])),\n",
       " Row(probability=DenseVector([0.0009, 0.8921, 0.0766, 0.0304])),\n",
       " Row(probability=DenseVector([0.0013, 0.8377, 0.1146, 0.0464])),\n",
       " Row(probability=DenseVector([0.0016, 0.7807, 0.1552, 0.0625])),\n",
       " Row(probability=DenseVector([0.0015, 0.7887, 0.1527, 0.0572])),\n",
       " Row(probability=DenseVector([0.0013, 0.8367, 0.1147, 0.0472])),\n",
       " Row(probability=DenseVector([0.0018, 0.7266, 0.1986, 0.073])),\n",
       " Row(probability=DenseVector([0.0015, 0.808, 0.1325, 0.0581])),\n",
       " Row(probability=DenseVector([0.0013, 0.8588, 0.1001, 0.0398])),\n",
       " Row(probability=DenseVector([0.0021, 0.685, 0.2123, 0.1006])),\n",
       " Row(probability=DenseVector([0.0018, 0.7623, 0.1655, 0.0704])),\n",
       " Row(probability=DenseVector([0.0022, 0.6568, 0.2456, 0.0954])),\n",
       " Row(probability=DenseVector([0.0018, 0.7406, 0.1778, 0.0798])),\n",
       " Row(probability=DenseVector([0.0016, 0.7788, 0.1598, 0.0598])),\n",
       " Row(probability=DenseVector([0.0017, 0.761, 0.1734, 0.0639])),\n",
       " Row(probability=DenseVector([0.0019, 0.7388, 0.1832, 0.0761])),\n",
       " Row(probability=DenseVector([0.0021, 0.7038, 0.2092, 0.0849])),\n",
       " Row(probability=DenseVector([0.0019, 0.714, 0.2017, 0.0825])),\n",
       " Row(probability=DenseVector([0.001, 0.8924, 0.0786, 0.0281])),\n",
       " Row(probability=DenseVector([0.0009, 0.8899, 0.0805, 0.0286])),\n",
       " Row(probability=DenseVector([0.002, 0.7149, 0.207, 0.0761])),\n",
       " Row(probability=DenseVector([0.0019, 0.7393, 0.1887, 0.0701])),\n",
       " Row(probability=DenseVector([0.0016, 0.7677, 0.1717, 0.059])),\n",
       " Row(probability=DenseVector([0.002, 0.7149, 0.207, 0.0761])),\n",
       " Row(probability=DenseVector([0.0021, 0.7307, 0.1879, 0.0793])),\n",
       " Row(probability=DenseVector([0.0017, 0.7629, 0.1705, 0.0649])),\n",
       " Row(probability=DenseVector([0.0016, 0.7931, 0.146, 0.0593])),\n",
       " Row(probability=DenseVector([0.003, 0.4913, 0.3531, 0.1526])),\n",
       " Row(probability=DenseVector([0.0018, 0.7795, 0.1535, 0.0652])),\n",
       " Row(probability=DenseVector([0.0018, 0.7405, 0.1703, 0.0874])),\n",
       " Row(probability=DenseVector([0.0015, 0.8083, 0.1301, 0.0602])),\n",
       " Row(probability=DenseVector([0.0022, 0.6756, 0.2267, 0.0955])),\n",
       " Row(probability=DenseVector([0.0013, 0.841, 0.1134, 0.0443])),\n",
       " Row(probability=DenseVector([0.0019, 0.7589, 0.1693, 0.07])),\n",
       " Row(probability=DenseVector([0.0014, 0.8147, 0.1343, 0.0496])),\n",
       " Row(probability=DenseVector([0.0018, 0.7522, 0.1779, 0.068])),\n",
       " Row(probability=DenseVector([0.0021, 0.1466, 0.4602, 0.391])),\n",
       " Row(probability=DenseVector([0.0023, 0.1767, 0.4108, 0.4102])),\n",
       " Row(probability=DenseVector([0.0021, 0.1191, 0.4515, 0.4273])),\n",
       " Row(probability=DenseVector([0.0025, 0.1137, 0.4514, 0.4324])),\n",
       " Row(probability=DenseVector([0.0022, 0.1053, 0.4511, 0.4414])),\n",
       " Row(probability=DenseVector([0.0025, 0.1618, 0.4189, 0.4168])),\n",
       " Row(probability=DenseVector([0.0023, 0.1796, 0.3861, 0.432])),\n",
       " Row(probability=DenseVector([0.003, 0.2279, 0.4237, 0.3454])),\n",
       " Row(probability=DenseVector([0.0022, 0.1309, 0.4702, 0.3966])),\n",
       " Row(probability=DenseVector([0.0027, 0.189, 0.3874, 0.4209])),\n",
       " Row(probability=DenseVector([0.0029, 0.1352, 0.4765, 0.3855])),\n",
       " Row(probability=DenseVector([0.0025, 0.1811, 0.397, 0.4194])),\n",
       " Row(probability=DenseVector([0.0024, 0.1039, 0.523, 0.3707])),\n",
       " Row(probability=DenseVector([0.0023, 0.1403, 0.4268, 0.4305])),\n",
       " Row(probability=DenseVector([0.0027, 0.2384, 0.3974, 0.3615])),\n",
       " Row(probability=DenseVector([0.0023, 0.1579, 0.45, 0.3899])),\n",
       " Row(probability=DenseVector([0.0025, 0.182, 0.3766, 0.4389])),\n",
       " Row(probability=DenseVector([0.0026, 0.19, 0.459, 0.3484])),\n",
       " Row(probability=DenseVector([0.002, 0.0583, 0.4681, 0.4716])),\n",
       " Row(probability=DenseVector([0.0027, 0.1623, 0.4594, 0.3757])),\n",
       " Row(probability=DenseVector([0.0023, 0.1541, 0.352, 0.4916])),\n",
       " Row(probability=DenseVector([0.0025, 0.1646, 0.4472, 0.3857])),\n",
       " Row(probability=DenseVector([0.002, 0.0715, 0.4524, 0.4741])),\n",
       " Row(probability=DenseVector([0.0024, 0.143, 0.4542, 0.4004])),\n",
       " Row(probability=DenseVector([0.0024, 0.1529, 0.4577, 0.387])),\n",
       " Row(probability=DenseVector([0.0023, 0.1459, 0.4528, 0.3991])),\n",
       " Row(probability=DenseVector([0.0021, 0.0956, 0.4788, 0.4235])),\n",
       " Row(probability=DenseVector([0.002, 0.0954, 0.4229, 0.4797])),\n",
       " Row(probability=DenseVector([0.0024, 0.1439, 0.4121, 0.4417])),\n",
       " Row(probability=DenseVector([0.0027, 0.2078, 0.46, 0.3294])),\n",
       " Row(probability=DenseVector([0.0027, 0.1544, 0.4627, 0.3802])),\n",
       " Row(probability=DenseVector([0.0027, 0.1691, 0.4706, 0.3575])),\n",
       " Row(probability=DenseVector([0.0026, 0.178, 0.4443, 0.3751])),\n",
       " Row(probability=DenseVector([0.0021, 0.0879, 0.4087, 0.5013])),\n",
       " Row(probability=DenseVector([0.0026, 0.1933, 0.3633, 0.4408])),\n",
       " Row(probability=DenseVector([0.0024, 0.2306, 0.3546, 0.4124])),\n",
       " Row(probability=DenseVector([0.0022, 0.1353, 0.4399, 0.4226])),\n",
       " Row(probability=DenseVector([0.0021, 0.0759, 0.4963, 0.4256])),\n",
       " Row(probability=DenseVector([0.0026, 0.2308, 0.3888, 0.3778])),\n",
       " Row(probability=DenseVector([0.0026, 0.1431, 0.435, 0.4193])),\n",
       " Row(probability=DenseVector([0.0026, 0.1518, 0.4332, 0.4124])),\n",
       " Row(probability=DenseVector([0.0024, 0.1616, 0.4181, 0.4178])),\n",
       " Row(probability=DenseVector([0.0026, 0.1549, 0.4548, 0.3877])),\n",
       " Row(probability=DenseVector([0.003, 0.1992, 0.4441, 0.3537])),\n",
       " Row(probability=DenseVector([0.0026, 0.1633, 0.4224, 0.4118])),\n",
       " Row(probability=DenseVector([0.0026, 0.2315, 0.4024, 0.3634])),\n",
       " Row(probability=DenseVector([0.0026, 0.1965, 0.4084, 0.3926])),\n",
       " Row(probability=DenseVector([0.0024, 0.1632, 0.4437, 0.3907])),\n",
       " Row(probability=DenseVector([0.003, 0.2427, 0.4172, 0.3371])),\n",
       " Row(probability=DenseVector([0.0026, 0.1816, 0.4191, 0.3967])),\n",
       " Row(probability=DenseVector([0.0016, 0.0591, 0.2778, 0.6615])),\n",
       " Row(probability=DenseVector([0.002, 0.0746, 0.3613, 0.562])),\n",
       " Row(probability=DenseVector([0.0015, 0.0453, 0.3776, 0.5756])),\n",
       " Row(probability=DenseVector([0.0019, 0.0737, 0.3833, 0.541])),\n",
       " Row(probability=DenseVector([0.0016, 0.0528, 0.336, 0.6095])),\n",
       " Row(probability=DenseVector([0.0013, 0.0298, 0.3867, 0.5822])),\n",
       " Row(probability=DenseVector([0.0025, 0.112, 0.3533, 0.5322])),\n",
       " Row(probability=DenseVector([0.0015, 0.0412, 0.4235, 0.5338])),\n",
       " Row(probability=DenseVector([0.0016, 0.0365, 0.4148, 0.5471])),\n",
       " Row(probability=DenseVector([0.0015, 0.062, 0.309, 0.6276])),\n",
       " Row(probability=DenseVector([0.002, 0.1001, 0.3641, 0.5338])),\n",
       " Row(probability=DenseVector([0.0018, 0.057, 0.3899, 0.5513])),\n",
       " Row(probability=DenseVector([0.0017, 0.0575, 0.3704, 0.5704])),\n",
       " Row(probability=DenseVector([0.0019, 0.0576, 0.3531, 0.5874])),\n",
       " Row(probability=DenseVector([0.0018, 0.056, 0.2974, 0.6448])),\n",
       " Row(probability=DenseVector([0.0018, 0.0764, 0.3214, 0.6004])),\n",
       " Row(probability=DenseVector([0.0019, 0.0805, 0.3916, 0.526])),\n",
       " Row(probability=DenseVector([0.0015, 0.0699, 0.3523, 0.5763])),\n",
       " Row(probability=DenseVector([0.001, 0.0131, 0.3617, 0.6242])),\n",
       " Row(probability=DenseVector([0.002, 0.053, 0.4464, 0.4986])),\n",
       " Row(probability=DenseVector([0.0016, 0.0566, 0.3392, 0.6026])),\n",
       " Row(probability=DenseVector([0.0021, 0.0888, 0.3377, 0.5714])),\n",
       " Row(probability=DenseVector([0.0012, 0.0234, 0.4081, 0.5673])),\n",
       " Row(probability=DenseVector([0.002, 0.0729, 0.4049, 0.5202])),\n",
       " Row(probability=DenseVector([0.0018, 0.0803, 0.3495, 0.5685])),\n",
       " Row(probability=DenseVector([0.0017, 0.0686, 0.4132, 0.5165])),\n",
       " Row(probability=DenseVector([0.0021, 0.088, 0.3959, 0.514])),\n",
       " Row(probability=DenseVector([0.0022, 0.1116, 0.3771, 0.5091])),\n",
       " Row(probability=DenseVector([0.0017, 0.0496, 0.354, 0.5947])),\n",
       " Row(probability=DenseVector([0.0017, 0.0666, 0.4518, 0.4798])),\n",
       " Row(probability=DenseVector([0.0014, 0.0347, 0.4223, 0.5416])),\n",
       " Row(probability=DenseVector([0.0016, 0.0846, 0.3927, 0.5211])),\n",
       " Row(probability=DenseVector([0.0016, 0.0456, 0.341, 0.6117])),\n",
       " Row(probability=DenseVector([0.0021, 0.0963, 0.4325, 0.4691])),\n",
       " Row(probability=DenseVector([0.0021, 0.0741, 0.436, 0.4878])),\n",
       " Row(probability=DenseVector([0.0013, 0.0291, 0.3757, 0.5939])),\n",
       " Row(probability=DenseVector([0.0018, 0.0835, 0.2925, 0.6222])),\n",
       " Row(probability=DenseVector([0.002, 0.0938, 0.3808, 0.5234])),\n",
       " Row(probability=DenseVector([0.0022, 0.119, 0.3724, 0.5065])),\n",
       " Row(probability=DenseVector([0.0017, 0.0651, 0.374, 0.5592])),\n",
       " Row(probability=DenseVector([0.0016, 0.0508, 0.3218, 0.6258])),\n",
       " Row(probability=DenseVector([0.0017, 0.0615, 0.3545, 0.5823])),\n",
       " Row(probability=DenseVector([0.002, 0.0746, 0.3613, 0.562])),\n",
       " Row(probability=DenseVector([0.0016, 0.0545, 0.3302, 0.6137])),\n",
       " Row(probability=DenseVector([0.0016, 0.0578, 0.3018, 0.6388])),\n",
       " Row(probability=DenseVector([0.0017, 0.0561, 0.3456, 0.5966])),\n",
       " Row(probability=DenseVector([0.0018, 0.051, 0.3979, 0.5493])),\n",
       " Row(probability=DenseVector([0.0019, 0.0762, 0.3725, 0.5495])),\n",
       " Row(probability=DenseVector([0.0019, 0.1, 0.3012, 0.5969])),\n",
       " Row(probability=DenseVector([0.0022, 0.1116, 0.363, 0.5232]))]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = vec.collect()\n",
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_row = vec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.0016, 0.7914, 0.1504, 0.0567])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0015715527683337984"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_row[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = vec_row[0].toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00157155, 0.79135577, 0.15035964, 0.05671303])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0015715527683337984"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[probability: vector]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_3_vec = pre_3.select('probability')\n",
    "pre_3_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-d66d17585432>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpre_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpre_3_vec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoArray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "pre_list = pre_3_vec[0].toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'probability'>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_3_vec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'probability[0]'>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_3_vec[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(probability=DenseVector([0.0016, 0.7914, 0.1504, 0.0567]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_3_vec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prob_3 = pre_3.select(['_c0', '_c1', 'probability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------------------+\n",
      "|_c0|_c1|         probability|\n",
      "+---+---+--------------------+\n",
      "|5.1|3.5|[0.00157155276833...|\n",
      "|4.9|3.0|[0.00215601550463...|\n",
      "|4.7|3.2|[0.00184854030564...|\n",
      "|4.6|3.1|[0.00201701076712...|\n",
      "|5.0|3.6|[0.00144326216378...|\n",
      "+---+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_prob_3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: double, _c1: double, probability: vector]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_prob_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'probability'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_prob_3.probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------------------+--------------------+\n",
      "|_c0|_c1|         probability|              prob_1|\n",
      "+---+---+--------------------+--------------------+\n",
      "|5.1|3.5|[0.00157155276833...|[0.7913557718676799]|\n",
      "|4.9|3.0|[0.00215601550463...|[0.6804221441922835]|\n",
      "|4.7|3.2|[0.00184854030564...|[0.7571145009018414]|\n",
      "|4.6|3.1|[0.00201701076712...|[0.7274824434397575]|\n",
      "|5.0|3.6|[0.00144326216378...|[0.8187212689762617]|\n",
      "+---+---+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vs = VectorSlicer(inputCol=\"probability\", outputCol=\"prob_1\", indices=[1])\n",
    "prob_1 = vs.transform(new_prob_3)\n",
    "prob_1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: double, _c1: double, probability: vector, prob_1: vector]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prob_1 = prob_1.select(['_c0', '_c1', 'prob_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------------------+\n",
      "|_c0|_c1|              prob_1|\n",
      "+---+---+--------------------+\n",
      "|5.1|3.5|[0.7913557718676799]|\n",
      "|4.9|3.0|[0.6804221441922835]|\n",
      "|4.7|3.2|[0.7571145009018414]|\n",
      "|4.6|3.1|[0.7274824434397575]|\n",
      "|5.0|3.6|[0.8187212689762617]|\n",
      "+---+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_prob_1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=5.1),\n",
       " Row(_c0=4.9),\n",
       " Row(_c0=4.7),\n",
       " Row(_c0=4.6),\n",
       " Row(_c0=5.0),\n",
       " Row(_c0=5.4),\n",
       " Row(_c0=4.6),\n",
       " Row(_c0=5.0),\n",
       " Row(_c0=4.4),\n",
       " Row(_c0=4.9),\n",
       " Row(_c0=5.4),\n",
       " Row(_c0=4.8),\n",
       " Row(_c0=4.8),\n",
       " Row(_c0=4.3),\n",
       " Row(_c0=5.8),\n",
       " Row(_c0=5.7),\n",
       " Row(_c0=5.4),\n",
       " Row(_c0=5.1),\n",
       " Row(_c0=5.7),\n",
       " Row(_c0=5.1),\n",
       " Row(_c0=5.4),\n",
       " Row(_c0=5.1),\n",
       " Row(_c0=4.6),\n",
       " Row(_c0=5.1),\n",
       " Row(_c0=4.8),\n",
       " Row(_c0=5.0),\n",
       " Row(_c0=5.0),\n",
       " Row(_c0=5.2),\n",
       " Row(_c0=5.2),\n",
       " Row(_c0=4.7),\n",
       " Row(_c0=4.8),\n",
       " Row(_c0=5.4),\n",
       " Row(_c0=5.2),\n",
       " Row(_c0=5.5),\n",
       " Row(_c0=4.9),\n",
       " Row(_c0=5.0),\n",
       " Row(_c0=5.5),\n",
       " Row(_c0=4.9),\n",
       " Row(_c0=4.4),\n",
       " Row(_c0=5.1),\n",
       " Row(_c0=5.0),\n",
       " Row(_c0=4.5),\n",
       " Row(_c0=4.4),\n",
       " Row(_c0=5.0),\n",
       " Row(_c0=5.1),\n",
       " Row(_c0=4.8),\n",
       " Row(_c0=5.1),\n",
       " Row(_c0=4.6),\n",
       " Row(_c0=5.3),\n",
       " Row(_c0=5.0),\n",
       " Row(_c0=7.0),\n",
       " Row(_c0=6.4),\n",
       " Row(_c0=6.9),\n",
       " Row(_c0=5.5),\n",
       " Row(_c0=6.5),\n",
       " Row(_c0=5.7),\n",
       " Row(_c0=6.3),\n",
       " Row(_c0=4.9),\n",
       " Row(_c0=6.6),\n",
       " Row(_c0=5.2),\n",
       " Row(_c0=5.0),\n",
       " Row(_c0=5.9),\n",
       " Row(_c0=6.0),\n",
       " Row(_c0=6.1),\n",
       " Row(_c0=5.6),\n",
       " Row(_c0=6.7),\n",
       " Row(_c0=5.6),\n",
       " Row(_c0=5.8),\n",
       " Row(_c0=6.2),\n",
       " Row(_c0=5.6),\n",
       " Row(_c0=5.9),\n",
       " Row(_c0=6.1),\n",
       " Row(_c0=6.3),\n",
       " Row(_c0=6.1),\n",
       " Row(_c0=6.4),\n",
       " Row(_c0=6.6),\n",
       " Row(_c0=6.8),\n",
       " Row(_c0=6.7),\n",
       " Row(_c0=6.0),\n",
       " Row(_c0=5.7),\n",
       " Row(_c0=5.5),\n",
       " Row(_c0=5.5),\n",
       " Row(_c0=5.8),\n",
       " Row(_c0=6.0),\n",
       " Row(_c0=5.4),\n",
       " Row(_c0=6.0),\n",
       " Row(_c0=6.7),\n",
       " Row(_c0=6.3),\n",
       " Row(_c0=5.6),\n",
       " Row(_c0=5.5),\n",
       " Row(_c0=5.5),\n",
       " Row(_c0=6.1),\n",
       " Row(_c0=5.8),\n",
       " Row(_c0=5.0),\n",
       " Row(_c0=5.6),\n",
       " Row(_c0=5.7),\n",
       " Row(_c0=5.7),\n",
       " Row(_c0=6.2),\n",
       " Row(_c0=5.1),\n",
       " Row(_c0=5.7),\n",
       " Row(_c0=6.3),\n",
       " Row(_c0=5.8),\n",
       " Row(_c0=7.1),\n",
       " Row(_c0=6.3),\n",
       " Row(_c0=6.5),\n",
       " Row(_c0=7.6),\n",
       " Row(_c0=4.9),\n",
       " Row(_c0=7.3),\n",
       " Row(_c0=6.7),\n",
       " Row(_c0=7.2),\n",
       " Row(_c0=6.5),\n",
       " Row(_c0=6.4),\n",
       " Row(_c0=6.8),\n",
       " Row(_c0=5.7),\n",
       " Row(_c0=5.8),\n",
       " Row(_c0=6.4),\n",
       " Row(_c0=6.5),\n",
       " Row(_c0=7.7),\n",
       " Row(_c0=7.7),\n",
       " Row(_c0=6.0),\n",
       " Row(_c0=6.9),\n",
       " Row(_c0=5.6),\n",
       " Row(_c0=7.7),\n",
       " Row(_c0=6.3),\n",
       " Row(_c0=6.7),\n",
       " Row(_c0=7.2),\n",
       " Row(_c0=6.2),\n",
       " Row(_c0=6.1),\n",
       " Row(_c0=6.4),\n",
       " Row(_c0=7.2),\n",
       " Row(_c0=7.4),\n",
       " Row(_c0=7.9),\n",
       " Row(_c0=6.4),\n",
       " Row(_c0=6.3),\n",
       " Row(_c0=6.1),\n",
       " Row(_c0=7.7),\n",
       " Row(_c0=6.3),\n",
       " Row(_c0=6.4),\n",
       " Row(_c0=6.0),\n",
       " Row(_c0=6.9),\n",
       " Row(_c0=6.7),\n",
       " Row(_c0=6.9),\n",
       " Row(_c0=5.8),\n",
       " Row(_c0=6.8),\n",
       " Row(_c0=6.7),\n",
       " Row(_c0=6.7),\n",
       " Row(_c0=6.3),\n",
       " Row(_c0=6.5),\n",
       " Row(_c0=6.2),\n",
       " Row(_c0=5.9)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_prob_1.select('_c0').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(_c0=5.1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_prob_1.select('_c0').collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_prob_1.select('_c0').collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(new_prob_1['prob_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'CAST(prob_1 AS STRING)'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_prob_1['prob_1'].cast('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(new_prob_1['prob_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "new2 = new_prob_1.select(['_c0', '_c1', new_prob_1['prob_1'].cast('string').alias('prob_1_str')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: double, _c1: double, prob_1_str: string]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(new2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "new2.toPandas().to_csv('C:\\Wang Hanmo\\scripts\\spark\\prob_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[prob_1_str: string]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_prob_1.select(new_prob_1['prob_1'].cast('string').alias('prob_1_str'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: double, _c1: double, prob_1: vector]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_prob_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-35-666840b5f7c2>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-35-666840b5f7c2>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    new2_prob_1 = new_prob_1.withColumn('prob_1', new_prob_1['prob_1'].collect()\u001b[0m\n\u001b[1;37m                                                                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "new2_prob_1 = new_prob_1.withColumn('prob_1', new_prob_1['prob_1'].collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prob_1.select('prob_1')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prob_1['prob_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prob_1.write.csv('C:\\Wang Hanmo\\scripts\\spark\\prob_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_pd = new_prob_1.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prob_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_pd.to_csv('C:\\Wang Hanmo\\scripts\\spark\\prob_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type(prob_pd['probability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_pd['probability'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(prob_pd['probability'][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVal(df):\n",
    "    return df[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prob_pd = prob_pd['probability'].apply(getVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_pd['prob_1'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(prob_pd['probability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_1.select('prob_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_1.prob_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_1.select('prob_1').collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_1.select('prob_1').collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_1.select('prob_1').collect()[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(prob_1.select('prob_1').collect()[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_1_csv = prob_1.select(['_c0', 'prob_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_1_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_1_df = prob_1_csv.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_1_df['prob_1'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prob_1_df['prob_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(prob_1_df['prob_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_1_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_1_csv.toPandas().to_csv('C:\\Wang Hanmo\\scripts\\spark\\prob_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_1_csv.write.csv('C:\\Wang Hanmo\\scripts\\spark\\prob_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prob_3 = new_prob_3.withColumn('prob_1', prob_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prob_3 = new_prob_3.withColumn('probability', new_prob_3.probability.getItem(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df = new_prob_3.select('probability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_c2 = pre_3.select('_c2')\n",
    "prob_c2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prob_3 = new_prob_3.withColumn('new_c2', prob_c2.getItem(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prob_3.withColumn('probability', prob_df.getItem(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tra_df = train_data.select(['_c0', '_c1'])\n",
    "tra_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.select('_c0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.select('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_1 = dataset['_c0']\n",
    "c_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_v1 = dataset._c0\n",
    "col_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_v2 = dataset._c1\n",
    "col_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_v12 = col_v1 * col_v2\n",
    "col_v12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = dataset.withColumn('col_v12', col_v12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "col1 = dataset.select('_c0')\n",
    "col1_pd = col1.toPandas()\n",
    "col1_ar = col1_pd.values\n",
    "col1_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "col2 = dataset.select('_c1')\n",
    "col2_pd = col2.toPandas()\n",
    "col2_ar = col2_pd.values\n",
    "col2_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coll = col1_ar * col2_ar\n",
    "coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll_df = pd.DataFrame(coll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type(coll_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll_spark_df = spark.createDataFrame(coll_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll_spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(coll_spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll_cl = coll_spark_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll_list = [i[0] for i in coll_cl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.select(['_c0', '_c1']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_delete = dataset.select([i for i in dataset.columns if i != 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_delete.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset._c0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = dataset.withColumn('_c00', dataset['_c0'] + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataset.select('_c0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_cl= dataset.select('_c0').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_list = [i[0] for i in dataset_cl]  # OK\n",
    "dataset_list = [i['_c0'] for i in dataset_cl]  # OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataset_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除某些无用列\n",
    "new_dataset = dataset.drop('_c0', '_c1', '_c2', '_c3', '_c4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新_模型训练\n",
    "lr_2 = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel_2 = lr_2.fit(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel_2.save('C:\\Wang Hanmo\\scripts\\spark\\lr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression.load('C:\\Wang Hanmo\\scripts\\spark\\lr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression.load('C:\\Wang Hanmo\\scripts\\spark\\lr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegressionModel.load('C:\\Wang Hanmo\\scripts\\spark\\lr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions4 = lr_model.transform(new_dataset, thresholds=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions4.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改阈值\n",
    "lr_model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.getParam('threshold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.getParam('thresholds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.isDefined('thresholds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.isSet('thresholds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model_read = lr_model.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model_read.set('threshold', 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.set(threshold, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_3 = lr_model.transform(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_2 = lrModel_2.transform(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction_2.show(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = prediction_2.select('label').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_list = prediction_2.select('probability').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(label_list[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [label_list[i][0] for i in range(0, len(label_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_list = prediction_2.select('probability').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_list[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_list = [prob_list[i][0] for i in range(0, len(prob_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list[0] >= 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in label_list:\n",
    "    print(type(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSummary = lrModel_2.summary\n",
    "trainingSummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSummary.falsePositiveRateByLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSummary.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSummary.precisionByLabel    # 按照0,1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSummary.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objectiveHistory = trainingSummary.objectiveHistory\n",
    "print(\"objectiveHistory:\")\n",
    "for objective in objectiveHistory:\n",
    "    print(objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSummary.roc.show()\n",
    "print(\"areaUnderROC: \" + str(trainingSummary.areaUnderROC))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testData) = dataset.randomSplit([0.8, 0.2], 123)\n",
    "# print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "# print(\"Test Dataset Count: \" + str(testData.count()))\n",
    "# trainingData.show(10)\n",
    "testData.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型预测\n",
    "predictions = lrModel.transform(testData)\n",
    "# print(prediction)\n",
    "predictions.show(10)\n",
    "\n",
    "# ROC score\n",
    "# evaluator = BinaryClassificationEvaluator()\n",
    "# evaluator.evaluate(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preSummary = prediction.summary\n",
    "preSummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preSummary.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preSummary.weightedRecall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|label|sentence|\n",
      "+-----+--------+\n",
      "|    1|     asf|\n",
      "|    5|    2143|\n",
      "|    3|    rfds|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentenceDataFrame = spark.createDataFrame((\n",
    "      (1, \"asf\"),\n",
    "      (5, \"2143\"),\n",
    "      (3, \"rfds\")\n",
    "    )).toDF(\"label\", \"sentence\")\n",
    "sentenceDataFrame.show()\n",
    "\n",
    "sentenceDataFrame1 = spark.createDataFrame((\n",
    "      (1, \"asf\"),\n",
    "      (2, \"2143\"),\n",
    "      (4, \"rfds\")\n",
    "    )).toDF(\"label\", \"sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = [sentenceDataFrame.sentence==sentenceDataFrame1.sentence, sentenceDataFrame.label==sentenceDataFrame1.label]\n",
    "result = sentenceDataFrame.join(sentenceDataFrame1, condition, 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-----+--------+\n",
      "|label|sentence|label|sentence|\n",
      "+-----+--------+-----+--------+\n",
      "|    1|     asf|    1|     asf|\n",
      "+-----+--------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([('100-200',)], ['str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[str: string]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|    str|\n",
      "+-------+\n",
      "|100-200|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dd = df.select(df.str.cast(\"float\").alias('f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[f: float]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|   f|\n",
      "+----+\n",
      "|null|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df.select(regexp_replace('str', '-', '').alias('str'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[str: string]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|   str|\n",
      "+------+\n",
      "|100200|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_df = df_new.select(df_new.d.cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[d: float]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|       d|\n",
      "+--------+\n",
      "|100200.0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
